import streamlit as st

import answered_app
import users_app

from pyspark.ml import PipelineModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, explode, col
from pyspark.sql.window import Window

def get_evaluator(labelCol):
    return MulticlassClassificationEvaluator(labelCol = labelCol, predictionCol="prediction")

def main(spark):
    PAGES = {
        'Answer Predictor': (answered_app, get_answers, answer_model),
        'User predictor': (users_app, get_users, user_model)
    }

    st.sidebar.title('Project')
    selection = st.sidebar.radio("Go to", list(PAGES.keys()))
    page, get_data, model = PAGES[selection]
    page.app(get_data(), spark, model)

def get_evaluator(labelCol):
    return MulticlassClassificationEvaluator(labelCol = labelCol, predictionCol="prediction")
    

    # Caching the output generated by the methods so that streamlit doesn't have to recompute the predictions and model upon
    # reload. Currently streamlit doesn't support caching certain pyspark related objects like PipelineModel and the sparse vectors
    # that are part of the features. Hence spark dataframes are converted to streamlit-serializable Pandas dataframes for use
    # in further computations.

@st.cache(suppress_st_warning=True, allow_output_mutation=True)
def get_answers():
    data = spark.read.parquet('ml-data/answer-data/')
    model = answer_model
    predictions = model.transform(data)
    predictions = predictions.select('id', 'reputation', 'title','tags','answered', 'prediction').cache()
    evaluator = get_evaluator('answered')
    score = evaluator.evaluate(predictions)
    prob = predictions.withColumn('tag', explode(predictions['tags'])).groupby(predictions['id'], col('tag')).agg(sum(predictions['answered']).alias('sum'))\
        .withColumn("percent", col('sum') /  sum('sum').over(Window.partitionBy('id')) * 100).toPandas()
    pred = predictions.toPandas()
    return (pred, prob, score)

@st.cache(suppress_st_warning=True, allow_output_mutation=True)
def get_users():
    data = spark.read.parquet('ml-data/users/').sample(0.01)
    random_user = data.first()['id']
    return random_user

if __name__ == '__main__':

    st.set_page_config(layout="wide")
    st.markdown(
        """
        <style>
        [data-testid="stSidebar"][aria-expanded="true"] > div:first-child {
            width: 145px;
        }
        [data-testid="stSidebar"][aria-expanded="false"] > div:first-child {
            width: 500px;
            margin-left: -500px;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )
    
    answer_model = PipelineModel.load('models/answer-predictor/')
    user_model = PipelineModel.load('models/user-predictor')
    spark = SparkSession.builder.appName('stack overflow predictor').getOrCreate()
    assert spark.version >= '3.0' # make sure we have Spark 3.0+
    sc = spark.sparkContext
    main(spark)
    pass
